{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setings\n",
    "\n",
    "Bibliotecas usadas:\n",
    "\n",
    "- datetime\n",
    "- pandas\n",
    "- matplotlib\n",
    "\n",
    "- sys\n",
    "- json\n",
    "- glob\n",
    "\n",
    "- webvtt\n",
    "\n",
    "Outros:\n",
    "\n",
    "- Youtube API (https://developers.google.com/youtube/v3)\n",
    "- Youtube-dl (https://youtube-dl.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurações básicas \n",
    "\n",
    "- Bibliotecas usadas\n",
    "- Diretórios e locais de trabalho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas usadas\n",
    "\n",
    "\"\"\"\n",
    "Caso não tenha algumas das bibliotecas instaladas digitar \"!pip install\". Exemplo: !pip install os. \n",
    "Exceção para os casos YoutubeAPI e Youtube-dl. Nesses casos, acessar os links indicados e verificar os documentos disponíveis. \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import webvtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretórios e locais de trabalho\n",
    "\n",
    "cwd = os.getcwd()\n",
    "#print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir diretórios e locais de trabalho\n",
    "\n",
    "BASE_DIR = os.path.dirname(cwd) # base de trabalho\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\") # dados gerais levantados durante projeto\n",
    "META_DIR = os.path.join(BASE_DIR, \"metadados\") #metadados levantados durante projeto\n",
    "CACHE_DIR = os.path.join(BASE_DIR, \"cache\") # material em processo - pode ser apagado ao final, caso julgue necessário\n",
    "LINK_DIR = os.path.join(BASE_DIR, \"links\") # material em processo - pode ser apagado ao final, caso julgue necessário\n",
    "SUB_DIR = os.path.join(CACHE_DIR, \"subs\") # local de trabalho para processamento de legenda\n",
    "VID_DIR = os.path.join(CACHE_DIR, \"vids\") # local de trabalho para processamento de legenda\n",
    "\n",
    "# Criar diretórios e locais de trabalho\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(META_DIR, exist_ok=True)\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(SUB_DIR, exist_ok=True)\n",
    "os.makedirs(VID_DIR, exist_ok=True)\n",
    "os.makedirs(LINK_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contexto\n",
    "now = datetime.datetime.now()\n",
    "year = datetime.datetime.now().year\n",
    "day = datetime.datetime.now().day\n",
    "month = datetime.datetime.now().month\n",
    "\n",
    "context = f\"_{month}-{day}-{year}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levantar metadados de vídeos de um canal\n",
    "\n",
    "- Selecionar link de corpus\n",
    "    - lives da semana\n",
    "    - canal de Jair Bolsonaro\n",
    "   \n",
    "- Levantar os metadados de cada vídeo - base: links de vídeos / ferramenta: youtube-dl\n",
    "- Criar documento único\n",
    "- Criar Data Frame\n",
    "- Salvar documento em .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecionar link de corpus\n",
    "\n",
    "- lives da semana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar função para selecionar link de corpus\n",
    "\n",
    "def link_graber(open_file, save_file):\n",
    "    df = pd.read_csv(f\"{DATA_DIR}/{open_file}\")\n",
    "    \n",
    "    links = df.Link\n",
    "    \n",
    "    links.to_csv(f\"{LINK_DIR}/{save_file}\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros para lives da semana em link_graber\n",
    "\n",
    "open_file = f\"JairChannel_corpusLives{context}.csv\"\n",
    "save_file = f\"JairChannel_corpusLives_LINKS{context}.csv\"\n",
    "\n",
    "link_graber(open_file, save_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecionar link de corpus\n",
    "- canal de Jair Bolsonaro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros para canal de Jair Bolsonaro em link_graber\n",
    "\n",
    "open_file = f\"JairChannel_corpus{context}.csv\"\n",
    "save_file = f\"JairChannel_corpusCanal_LINKS{context}.csv\"\n",
    "\n",
    "link_graber(open_file, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadados - vídeos de Lives\n",
    "\n",
    "- Levantar os metadados de cada vídeo \n",
    "- base: \"JairChannel_corpusLives_LINKS{context}.csv\"\n",
    "- ferramenta: youtube-dl (escolhemos usar essa ferramenta pela facilidade e por levantar dados que não são incluídos na api do youtube (v3), commo tags por exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JairChannel_corpusLives_LINKS{context}.csv\n",
    "\n",
    "title = f\"JairChannel_corpusLives_LINKS{context}.csv\"\n",
    "df = pd.read_csv(f\"{LINK_DIR}/{title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Levantar no diretório de cache todos metadados de cada vídeo - youtube-dl \n",
    "# youtube-dl: de um arquivo  (com links de vídeos do youtube) [-a] baixar info em json [--write-info-json], ignorar erros [-i], não baixar os vídeos em si [--skip-download], salvar os arquivos e nomeá-los com o id do vídeo [--id]\n",
    "\n",
    "processing_dir = SUB_DIR\n",
    "\n",
    "os.chdir(processing_dir)\n",
    "os.system(f\"youtube-dl -a {LINK_DIR}/{title} --id --write-info-json -i --skip-download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2627"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar documento único\n",
    "## listar em diretório todos os documentos terminados em '.json'\n",
    "\n",
    "json_dir = processing_dir\n",
    "json_root = os.path.join(json_dir, \"*.json\")\n",
    "file_list = glob.glob(json_root)\n",
    "\n",
    "# len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar documento único\n",
    "## 1. a partir de lista de documentos '.json' estabelecer um único documento json - 'merge_file2021.json'\n",
    "\n",
    "results = []\n",
    "\n",
    "for f in file_list: \n",
    "    with open(f, 'r') as infile:\n",
    "        json_file = json.load(infile)\n",
    "        results.append(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar documento único\n",
    "## 2. a partir de lista de documentos '.json' estabelecer um único documento json - 'merge_file2021.json'\n",
    "\n",
    "with open('merge_file2021.json', 'w') as outfile:\n",
    "    json.dump(results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar documento único\n",
    "## 3. a partir de lista de documentos '.json' estabelecer um único documento json - 'merge_file2021.json'\n",
    "\n",
    "with open(\"merge_file2021.json\") as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-a79c850bc22b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mvideoId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0muploader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uploader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0muploader_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uploader_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Criar Data Frame\n",
    "## a partir do documento json - 'merge_file2021.json', criar Data Frame\n",
    "### key value pairs / json: videoId, uploader, uploader_id, uploader_url, channel_id, channel_url, upload_date, license, creator, title, description, categories, tags, duration, age_limit, annotations, chapters, videoLink, view_count, like_count, dislike_count, average_rating, is_live, start_time, end_time, series, playlist, playlist_index\n",
    "## salvar Data Frame na pasta metadados - META_DIR\n",
    "\n",
    "df_merged_data = []\n",
    "\n",
    "for d in data:\n",
    "        videoId = d['id']\n",
    "        uploader = str.lower(d['uploader'])\n",
    "        uploader_id = d['uploader_id']\n",
    "        uploader_url = d['uploader_url']\n",
    "        channel_id = d['channel_id']\n",
    "        channel_url = d['channel_url']\n",
    "        upload_date = d['upload_date']\n",
    "        license = d['license']\n",
    "        creator = d['creator']\n",
    "        title = str.lower(d['title'])\n",
    "        description = str.lower(d['description'])\n",
    "        categories = d['categories']\n",
    "        tags = d['tags']\n",
    "        duration = d['duration']\n",
    "        age_limit = d['age_limit']\n",
    "        annotations = d['annotations']\n",
    "        chapters = d['chapters']\n",
    "        videoLink = d['webpage_url']\n",
    "        view_count = d['view_count']\n",
    "        like_count = d['like_count']\n",
    "        dislike_count = d['dislike_count']\n",
    "        average_rating = d['average_rating']\n",
    "        is_live = d['is_live']\n",
    "        start_time = d['start_time']\n",
    "        end_time = d['end_time']\n",
    "        series = d['series']\n",
    "        playlist = d['playlist']\n",
    "        playlist_index = d['playlist_index']\n",
    "        \n",
    "        df_merged_data.append((videoId, uploader, uploader_id, uploader_url, channel_id, channel_url, upload_date, license, creator, title, description, categories, tags, duration, age_limit, annotations, chapters, videoLink, view_count, like_count, dislike_count, average_rating, is_live, start_time, end_time, series, playlist, playlist_index))\n",
    "        \n",
    "df_merged_data = pd.DataFrame(df_merged_data, columns=['videoId', 'uploader', 'uploader_id', 'uploader_url', 'channel_id', 'channel_url', 'upload_date', 'license', 'creator', 'title', 'description', 'categories', 'tags', 'duration', 'age_limit', 'annotations', 'chapters', 'videoLink', 'view_count', 'like_count', 'dislike_count', 'average_rating', 'is_live', 'start_time', 'end_time', 'series', 'playlist', 'playlist_index'])\n",
    "df_merged_data.upload_date = pd.to_datetime(df_merged_data.upload_date)\n",
    "\n",
    "df_merged_data.to_csv(f\"{META_DIR}/LivesBolsonaro_Metadado{context}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verificar documento criado\n",
    "title = f\"LivesBolsonaro_Metadado{context}.csv\"\n",
    "df = pd.read_csv(f\"{META_DIR}/{title}\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadados -  de todos os vídeos de canal de Jair bolsonaro (não só as lives)\n",
    "\n",
    "- **Apagar em cache os documentos anteriores**\n",
    "\n",
    "- Levantar os metadados de cada vídeo do canal\n",
    "- base: \"JairChannel_corpusCanal_LINKS{context}.csv\"\n",
    "- ferramenta: youtube-dl (escolhemos usar essa ferramenta pela facilidade e por levantar dados que não são incluídos na api do youtube (v3), commo tags por exemplo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apagar em cache os documentos anteriores**\n",
    "\n",
    "**Atenção**: Se desejar mantar os arquivos json com metadados individuais, pular próxima célula "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apagar em cache os documentos anteriores\n",
    "\n",
    "dircache = os.chdir(SUB_DIR)\n",
    "RESET = os.listdir(dircache) \n",
    "\n",
    "for item in RESET:\n",
    "    if item.endswith(\".json\"):\n",
    "        os.remove(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JairChannel_corpusCanal_LINKS{context}.csv\n",
    "\n",
    "title = f\"JairChannel_corpusCanal_LINKS{context}.csv\"\n",
    "df = pd.read_csv(f\"{LINK_DIR}/{title}\")\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Levantar no diretório de cache todos metadados de cada vídeo - youtube-dl \n",
    "# youtube-dl: de um arquivo  (com links de vídeos do youtube) [-a] baixar info em json [--write-info-json], ignorar erros [-i], não baixar os vídeos em si [--skip-download], salvar os arquivos e nomeá-los com o id do vídeo [--id]\n",
    "\n",
    "processing_dir = SUB_DIR\n",
    "os.chdir(processing_dir)\n",
    "os.system(f\"youtube-dl -a {LINK_DIR}/{title} --id --write-info-json -i --skip-download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar documento único\n",
    "## listar em diretório todos os documentos terminados em '.json'\n",
    "\n",
    "json_dir = SUB_DIR\n",
    "json_root = os.path.join(json_dir, \"*.json\")\n",
    "file_list = glob.glob(json_root)\n",
    "# print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar documento único\n",
    "## 1. a partir de lista de documentos '.json' estabelecer um único documento json - 'merge_file2021.json'\n",
    "\n",
    "results = []\n",
    "\n",
    "for f in file_list: \n",
    "    with open(f, 'r') as infile:\n",
    "        json_file = json.load(infile)\n",
    "        results.append(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar documento único\n",
    "## 2. a partir de lista de documentos '.json' estabelecer um único documento json - 'merge_file2021.json'\n",
    "\n",
    "with open('merge_file2021.json', 'w') as outfile:\n",
    "    json.dump(results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar documento único\n",
    "## 3. a partir de lista de documentos '.json' estabelecer um único documento json - 'merge_file2021.json'\n",
    "\n",
    "with open(\"merge_file2021.json\") as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-b45d785e2db5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mvideoId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0muploader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uploader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0muploader_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'uploader_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Criar Data Frame\n",
    "## a partir do documento json - 'merge_file2021.json', criar Data Frame\n",
    "### key value pairs / json: videoId, uploader, uploader_id, uploader_url, channel_id, channel_url, upload_date, license, creator, title, description, categories, tags, duration, age_limit, annotations, chapters, videoLink, view_count, like_count, dislike_count, average_rating, is_live, start_time, end_time, series, playlist, playlist_index\n",
    "## salvar Data Frame na pasta metadados - META_DIR\n",
    "\n",
    "df_merged_data = []\n",
    "\n",
    "for d in data:\n",
    "        videoId = d['id']\n",
    "        uploader = str.lower(d['uploader'])\n",
    "        uploader_id = d['uploader_id']\n",
    "        uploader_url = d['uploader_url']\n",
    "        channel_id = d['channel_id']\n",
    "        channel_url = d['channel_url']\n",
    "        upload_date = d['upload_date']\n",
    "        license = d['license']\n",
    "        creator = d['creator']\n",
    "        title = str.lower(d['title'])\n",
    "        description = str.lower(d['description'])\n",
    "        categories = d['categories']\n",
    "        tags = d['tags']\n",
    "        duration = d['duration']\n",
    "        age_limit = d['age_limit']\n",
    "        annotations = d['annotations']\n",
    "        chapters = d['chapters']\n",
    "        videoLink = d['webpage_url']\n",
    "        view_count = d['view_count']\n",
    "        like_count = d['like_count']\n",
    "        dislike_count = d['dislike_count']\n",
    "        average_rating = d['average_rating']\n",
    "        is_live = d['is_live']\n",
    "        start_time = d['start_time']\n",
    "        end_time = d['end_time']\n",
    "        series = d['series']\n",
    "        playlist = d['playlist']\n",
    "        playlist_index = d['playlist_index']\n",
    "        \n",
    "        df_merged_data.append((videoId, uploader, uploader_id, uploader_url, channel_id, channel_url, upload_date, license, creator, title, description, categories, tags, duration, age_limit, annotations, chapters, videoLink, view_count, like_count, dislike_count, average_rating, is_live, start_time, end_time, series, playlist, playlist_index))\n",
    "        \n",
    "df_merged_data = pd.DataFrame(df_merged_data, columns=['videoId', 'uploader', 'uploader_id', 'uploader_url', 'channel_id', 'channel_url', 'upload_date', 'license', 'creator', 'title', 'description', 'categories', 'tags', 'duration', 'age_limit', 'annotations', 'chapters', 'videoLink', 'view_count', 'like_count', 'dislike_count', 'average_rating', 'is_live', 'start_time', 'end_time', 'series', 'playlist', 'playlist_index'])\n",
    "df_merged_data.upload_date = pd.to_datetime(df_merged_data.upload_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_data.to_csv(f\"{META_DIR}/CanalBolsonaro_Metadado{context}.csv\")\n",
    "\n",
    "#verificar documento criado\n",
    "title = f\"CanalBolsonaro_Metadado{context}.csv\"\n",
    "df = pd.read_csv(f\"{META_DIR}/{title}\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apagar em cache os documentos anteriores\n",
    "\n",
    "dircache = os.chdir(SUB_DIR)\n",
    "RESET = os.listdir(dircache) \n",
    "\n",
    "for item in test:\n",
    "    if item.endswith(\".json\"):\n",
    "        os.remove(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorar documentos \n",
    "\n",
    "- documento criados: LivesBolsonaro e CanalBolsonaro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "CanalBolsonaro = f\"CanalBolsonaro_Metadado{context}.csv\"\n",
    "Canal_df = pd.read_csv(f\"{META_DIR}/{CanalBolsonaro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2610, 29)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Canal_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "LivesBolsonaro = f\"LivesBolsonaro_Metadado{context}.csv\"\n",
    "Live_df = pd.read_csv(f\"{META_DIR}/{LivesBolsonaro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 29)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Live_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As tags de Bolsonaro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dumoura/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "ctags = Canal_df.dropna(subset=['tags'])\n",
    "ctag = ctags.loc[:, ~ctags.columns.str.contains('^Unnamed')]\n",
    "ctag.drop(['uploader','videoId', 'uploader_id', 'uploader_url', 'channel_id', 'channel_url', 'license', 'creator', 'duration', 'age_limit', 'annotations', 'chapters', 'is_live', 'start_time', 'end_time','series', 'playlist', 'playlist_index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ctags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtags = Live_df.dropna(subset=['tags'])\n",
    "dtag = dtags.loc[:, ~dtags.columns.str.contains('^Unnamed')]\n",
    "dtag.drop(['uploader','videoId', 'uploader_id', 'uploader_url', 'channel_id', 'channel_url', 'license', 'creator', 'duration', 'age_limit', 'annotations', 'chapters', 'is_live', 'start_time', 'end_time','series', 'playlist', 'playlist_index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dumoura/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "dtag.drop(['uploader','videoId', 'uploader_id', 'uploader_url', 'channel_id', 'channel_url', 'license', 'creator', 'duration', 'age_limit', 'annotations', 'chapters', 'is_live', 'start_time', 'end_time','series', 'playlist', 'playlist_index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
